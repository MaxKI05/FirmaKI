import os
import sys
import streamlit as st

# â”€â”€â”€ Sicherstellen, dass tiktoken installiert ist (sonst freundliche Fehlermeldung) â”€â”€â”€
try:
    import tiktoken
except ImportError:
    st.set_page_config(page_title="Fehler", layout="centered")
    st.title("ğŸ“¦ Fehlendes Paket: tiktoken")
    st.error("Bitte installiere das Python-Paket 'tiktoken' mit: pip install tiktoken")
    st.stop()

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# â”€â”€â”€ HARTCODED API-KEY (wie gewÃ¼nscht) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key = st.secrets["OPENAI_API_KEY"]

# â”€â”€â”€ Prompt fÃ¼r den Map-Schritt (Einzelantwort pro Chunk) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
QUESTION_PROMPT_TEMPLATE = """
Du bist ein freundlicher, hilfsbereiter Assistent.
Deine Aufgabe ist es, Nutzer:innen ohne Vorwissen einfache, prÃ¤zise und verstÃ¤ndliche Antworten
auf Fragen zum Betreiberleitfaden zu geben. Vermeide Fachjargon und erklÃ¤re wenn nÃ¶tig.

Nutze ausschlieÃŸlich den folgenden Textauszug als Informationsquelle:
{context}

Frage:
{question}

Antwort:
"""

# â”€â”€â”€ Prompt fÃ¼r den Combine-Schritt (ZusammenfÃ¼hrung) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Verwende 'summaries' statt 'input_documents' fÃ¼r die Default document_variable_name
COMBINE_PROMPT_TEMPLATE = """
Du bist ein hilfreicher Assistent.
Du bekommst mehrere kurze Antworten (Summaries) zu einer Frage und sollst sie zu einer prÃ¤zisen Gesamtantwort zusammenfassen.

Frage:
{question}

Summaries:
{summaries}

Fasse sie nun verstÃ¤ndlich und vollstÃ¤ndig zusammen:
"""

@st.cache_resource(show_spinner=False)
def load_chain():
    # 1) Embeddings-Modell auf CPU laden (kein Meta-Tensor)
    embedding_model = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"}
    )

    # 2) FAISS-Index lokal laden
    vectorstore = FAISS.load_local(
        "leitfaden_index",
        embedding_model,
        allow_dangerous_deserialization=True
    )

    # 3) Retriever mit maximaler Marginal Relevance fÃ¼r diversere Treffer
    retriever = vectorstore.as_retriever(search_kwargs={
        "k": 5,                          # Anzahl finaler Kontexte
        "fetch_k": 20,                   # rohe Kandidaten
        "maximal_marginal_relevance": True,
        "lambda_mult": 0.5               # trade-off zwischen Relevanz und DiversitÃ¤t
    })

    # 4) Chat-LLM (schnelles Modell)
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.0
    )

    # 5) PromptTemplates definieren
    question_prompt = PromptTemplate(
        template=QUESTION_PROMPT_TEMPLATE,
        input_variables=["context", "question"]
    )

    combine_prompt = PromptTemplate(
        template=COMBINE_PROMPT_TEMPLATE,
        input_variables=["question", "summaries"]
    )

    # 6) RetrievalQA-Chain mit Map-Reduce
    chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="map_reduce",
        chain_type_kwargs={
            "question_prompt": question_prompt,
            "combine_prompt": combine_prompt
        }
    )

    return chain


def main():
    # Streamlit-Konfiguration
    st.set_page_config(
        page_title="PDF Chatbot",
        layout="centered"
    )

    st.title("ğŸ“˜ Frag den Betreiberleitfaden")
    st.markdown(
        "ğŸ’¡ **Beispiel-Fragen:**\n"
        "- Was muss ich vor dem Einschalten beachten?\n"
        "- Welche Aufgaben hat der Betreiber?\n"
        "- Was steht zur SicherheitsprÃ¼fung?"
    )

    # Formular mit Textarea und Submit-Button
    with st.form(key="frage_form", clear_on_submit=True):
        question = st.text_area(
            label="â“ Deine Frage:",
            height=150
        )
        submitted = st.form_submit_button(label="Antwort anzeigen")

    if submitted and question.strip():
        qa_chain = load_chain()
        with st.spinner("ğŸ“š Ich durchsuche den Leitfaden..."):
            try:
                answer = qa_chain.run(question)
            except Exception as e:
                st.error(f"ğŸ”´ Ein Fehler ist aufgetreten: {e}")
            else:
                if answer:
                    st.success("âœ… Antwort gefunden:")
                    st.write(answer)
                else:
                    st.error("âš ï¸ Leider konnte ich dazu im Leitfaden nichts finden.")

if __name__ == "__main__":
    main()


